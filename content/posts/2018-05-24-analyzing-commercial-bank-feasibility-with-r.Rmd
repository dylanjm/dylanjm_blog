---
title: Analyzing Commercial Bank Feasibility with R
author: Dylan McDowell
date: '2018-05-24'
slug: analyzing-commercial-bank-feasibility-with-r
categories:
  - Posts
tags:
  - economics
  - tidyverse
  - census
  - bls
  - finance
---

# Intro

Lately I have been doing some consulting work on the topic of commercial bank feasibility in the state of Idaho. My client is interested in looking at potential under-serviced markets they could expand to. While I won't publish specific data concerning my client, I thought this could be a good opportunity to play with some __public__ data published by the [Bureau of Labor Statistics](https://www.bls.gov/), [US Census Bureau](https://www.census.gov/), [FDIC](https://www.fdic.gov/) and others.

In this analysis I will rely heavily on `library(purrr)`, `library(blscrapeR)` and `library(tidycensus)`. I will also, at some point, need to utilize `library(rlang)`'s `enquo()` and `!!`. This is so I can keep my code reproducible and concise. For more information about how to use `library(rlang)`, you should first consult [Programming with dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html) for an introduction to the topic. While it may seem daunting at first, it's obvious to me that this is a powerful tool all data scientists should have in their toolbox. 

Thanks for reading and if you have any questions please feel free to at my twitter handle: [\@dylanjm_ds](https://twitter.com/dylanjm_ds)

# All I See are Quotients

Here are the libraries we'll need for this analytical foray.
```{r, message=FALSE}
library(blscrapeR)
library(tidyverse)
library(albersusa)
library(ggalt)
library(wesanderson)
library(patchwork)
library(cowplot)
library(tidycensus)
```

One of the first things that pops into my mind when thinking about potential market opportunities is the _Location Quotient_. The Location Quotient is defined as follows:

$$
LQ = \dfrac{e_{i}/e}{E_{i}/E}
$$

Where:

$$
e_{i} = \text{regional employment in sector} i \\
e = \text{total regional employment} \\
E_{i} = \text{national employment in sector} i \\
E = \text{total national employment}
$$

This ratio can provide a lot of insight into the saturation of a particular sector for any given area of the United States. Typically, as ratios go, anything higher than 1 starts to exhibit over-saturation for a given market, and anything less than 1 displays market under-utilization. What I want to do is look at the Location Quotient for  Commercial Banking, Credit Unions, and Mortgage Loan Brokers (NIACS codes 5221, 52213, 52231 respectively) for each county in Idaho.  

Let's write a function in combination with `library(blsrapeR)` to get all the data we need at one time from the Quarterly Census of Employment and Wages from the BLS.  

```{r}
# The original blscrapeR::qcew_api() is not really suited to take multiple years
# or niac codes at once. Here is a function that grabs it all!
get_industry_qcew <- function(years, niac_codes, qtr = "a"){
  # We want the years to repeat for each industry we're looking for
  year_rep <- rep(years, length(niac_codes))
  # We also want each code to be repeated for each year of interest
  niac_each <- rep(niac_codes, each = length(years))
  
  # Utilize purrr::map2() and purrr::possibly() to either grab our data 
  # or carry on when it hits an error
  industry_dat <- map2(year_rep, niac_each,
                       possibly(
                         ~ qcew_api(year = .x, qtr = qtr, 
                                    slice = "industry", sliceCode = .y),
                         otherwise = NULL
                       )
  ) %>% 
    compact() %>% # remove any NULL values from our list
    map_df(bind_rows) # create one large data set 
  return(industry_dat)
}
```

The above function has the ability to take multiple years and multiple industry codes and return the proper data. If you remember in one of my previous blog posts, [Using purrr to wrangle and clean economic data](https://uninformedpriors.org/posts/using-purrr-to-wrangle-and-clean-economic-data/) I had to utilize the `possibly()` function again, to ensure that my function would not stop when gathering the data. Since I'm only interested in annual averages right now, I'll go ahead and run the function:

```{r, eval=FALSE}
qcew_dat <- get_industry_qcew(2013:2016, c(5221, 52213, 52231))
```

A quick `glimpse()` at the data show the number of observations and variables I'll be working with.
```{r, echo=FALSE}
qcew_dat <- read_rds(here::here("data/qcew_dat.rds"))
glimpse(qcew_dat)
```

According to the [field layouts guide](https://data.bls.gov/cew/doc/layouts/csv_annual_layout.htm) at the BLS website. `lq_annual_avg_estabs` is the variable I am looking for. I'm looking to make a heat map by county for all the LQ data. I'm going to go ahead and just clean up this data a little bit:

```{r}
qcew_dat_clean <- qcew_dat %>% 
  # Reorder columns and drop unwanted variables
  select(area_fips, year, qtr, 
         industry_code, everything(),
         -own_code, -agglvl_code, -size_code, -disclosure_code,
         -lq_disclosure_code, -oty_disclosure_code)
```

Now, I want to make my data set interpretable so that when I give it to my client the file is easily digestable. So I'm going to create a County and State column by merging my data with another predefined data.frame that comes with `library(blscrapeR)`. I am also going to create a column that gives a verbal description of the respective NIACS code. This will make titling my plots easier later. 

```{r, warning=FALSE}
idaho_dat <- qcew_dat_clean %>% 
  left_join(area_titles, by = "area_fips") %>% 
  separate(area_title, c("county", "state"), sep = ",") %>% 
  mutate_at(vars(state, industry_code), trimws) %>% 
  left_join(niacs, by = "industry_code") %>% 
  mutate_at(vars(industry_title), as.character) %>% 
  select(area_fips, state, county, 
         year, qtr, industry_code, industry_title, everything()) %>% 
  filter(state %in% "Idaho")
```

At this point, I'm dying to just get some plots printed, (let's be real - I iteratively wrote this code so that it would be as concise as possible, I've already seen 1000 plots of this data, but let's suspend our disbelief as to make it feel natural) What I need to do now is leverage `library(alberusa)` to set up my mapping data.

```{r}
# This will give me all of the lat/long geometry's for each county in Idaho
us <- counties_composite()
us_map <- broom::tidy(us, region = "fips") %>% 
  filter(id %in% idaho_dat$area_fips)
```

Let's take a look at how the Commercial Banking (Depository Credit Intermediation) LQ has done over the last few years in Idaho:

```{r, warning = FALSE, fig.align='center', fig.height=6, fig.width=10}
com_banking <- idaho_dat %>% 
  filter(industry_code %in% "5221")

ggplot() +
    geom_map(data = us_map, map = us_map,
             aes(long, lat, map_id = id),
             color = "black", fill = NA) + 
    geom_map(data = com_banking, map = us_map,
             aes(fill = lq_annual_avg_estabs, map_id = area_fips)) + 
    scale_fill_gradientn(colors = wes_palettes$Zissou1) +
    coord_map() + 
    facet_grid(.~ year) + 
    labs(title = glue::glue("{tools::toTitleCase(com_banking$industry_title)}"),
         fill = "LQ", x = NULL, y = NULL) +
    theme_minimal() +
    theme(strip.text = element_text(face = "bold"),
          axis.title = element_blank(),
          axis.text = element_blank(),
          legend.title = element_text(face = "bold"),
          plot.title = element_text(family = "Helvetica Bold Oblique"),
          plot.margin=grid::unit(c(0,0,0,0), "mm"))
```

Wow, okay so 